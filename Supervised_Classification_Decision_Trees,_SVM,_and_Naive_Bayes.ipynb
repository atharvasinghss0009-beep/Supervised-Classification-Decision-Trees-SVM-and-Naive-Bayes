{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. What is Information Gain, and how is it used in Decision Trees?\n",
        "\n",
        "\n",
        "Ans:Information Gain in Decision Trees\n",
        "\n",
        "\n",
        "Information Gain is the primary metric used to determine which feature should be used to split data at each node of a decision tree. It measures the reduction in Entropy (randomness) achieved by partitioning a dataset based on a specific attribute.\n",
        "\n",
        "\n",
        "How It Is Used\n",
        "\n",
        " 1. Calculate Parent Entropy: The algorithm first calculates the entropy of the current dataset (the \"parent\" node) to determine how mixed the target classes are.\n",
        " 2. Evaluate Potential Splits: For every available feature, the algorithm simulates a split and calculates the weighted average entropy of the resulting \"child\" nodes.\n",
        " 3. Measure Gain: It subtracts the child entropy from the parent entropy:$$IG(S, A) = \\text{Entropy(Parent)} - \\text{Entropy(Children given Feature A)}$$\n",
        " 4. Select the Best Feature: The feature that results in the highest Information Gain (the largest reduction in uncertainty) is selected as the decision node.\n",
        " 5. Recursion: This process repeats for each branch until the data is perfectly classified or a stopping criterion is met."
      ],
      "metadata": {
        "id": "khv53kntoVuc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.What is the difference between Gini Impurity and Entropy?\n",
        "\n",
        "\n",
        "Ans:Key Differences at a Glance\n",
        "\n",
        "1. Computational Efficiency\n",
        "\n",
        "\n",
        " - Gini: Does not require calculating logarithms. This makes it significantly faster to compute, which is a major advantage when dealing with massive datasets or many features.\n",
        " - Entropy: Logarithmic calculations are more \"expensive\" for a processor. In large-scale applications, this can lead to slightly longer training times.\n",
        "\n",
        "\n",
        "2. Sensitivity\n",
        "\n",
        " - Gini: Tends to isolate the most frequent class in its own branch.\n",
        " - Entropy: Tends to produce more balanced trees. Because the log function \"punishes\" small probabilities more harshly than the squared function in Gini, Entropy is slightly more sensitive to changes in class probabilities.\n",
        "\n",
        "\n",
        "3. Practical Impact\n",
        "\n",
        "\n",
        "In 90% of real-world scenarios, the choice between the two won't drastically change your model's performance. They are highly correlated. However, Gini is the default in many libraries (like Scikit-Learn) simply because it is faster."
      ],
      "metadata": {
        "id": "fWKihB2ZrBnc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.What is Pre-Pruning in Decision Trees?\n",
        "\n",
        "\n",
        "Ans:Pre-pruning is a technique used to prevent overfitting in decision trees by halting the tree's growth before it perfectly fits the training data. It is often referred to as early stopping.\n",
        "\n",
        "\n",
        "Instead of allowing the tree to grow until every leaf is \"pure,\" pre-pruning applies specific constraints at each node. If a node does not meet these criteria, it stops splitting and becomes a terminal leaf.\n",
        "\n",
        "\n",
        "Common Pre-Pruning Criteria\n",
        "\n",
        " - Maximum Depth: Setting a limit on how many levels the tree can grow.\n",
        " - Minimum Samples per Split: Requiring a minimum number of data points to be present in a node before it is allowed to branch out.\n",
        " - Minimum Samples per Leaf: Ensuring that every final leaf contains at least a specific number of observations.\n",
        " - Information Gain Threshold: Only allowing a split if it improves the model's purity (e.g., Gini Impurity or Entropy) by a predefined minimum amount.\n",
        "\n",
        "\n",
        "Key Trade-offs\n",
        "\n",
        " - Advantage: It is computationally efficient because it reduces the time and memory needed to build the tree.\n",
        " - Disadvantage: It carries the risk of underfitting. It may stop a split that seems insignificant now but could have led to important patterns deeper in the tree (the \"Horizon Effect\")."
      ],
      "metadata": {
        "id": "NW2NhCOjs8XD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.Write a Python program to train a Decision Tree Classifier using Gini\n",
        "Impurity as the criterion and print the feature importances (practical).\n",
        "\n",
        "Ans:Training a Decision Tree Classifier using Scikit-Learn is a straightforward process. The Gini impurity criterion helps the tree decide how to split data by measuring the \"purity\" of a node (i.e., how often a randomly chosen element from the set would be incorrectly labeled).\n",
        "\n",
        "\n",
        "Here is a practical implementation using the built-in Iris dataset."
      ],
      "metadata": {
        "id": "2nb-E9cCuL5Z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FSLgF7TNoLQe"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import pandas as pd\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Initialize and train the Decision Tree Classifier\n",
        "# Using criterion='gini' as requested\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "clf.fit(X, y)\n",
        "\n",
        "# Access .feature_importances_\n",
        "importances = clf.feature_importances_\n",
        "\n",
        "# Print the results\n",
        "print(\"Decision Tree Feature Importances (Gini):\")\n",
        "for name, importance in zip(iris.feature_names, importances):\n",
        "    print(f\"{name}: {importance:.4f}\")\n",
        "\n",
        "\"\"\"\n",
        "Output:\n",
        "Decision Tree Feature Importances (Gini):\n",
        "sepal length (cm): 0.0133\n",
        "sepal width (cm): 0.0000\n",
        "petal length (cm): 0.5641\n",
        "petal width (cm): 0.4226\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.What is a Support Vector Machine (SVM)?\n",
        "\n",
        "Ans:A Support Vector Machine (SVM) is a supervised machine learning algorithm used for classification and regression. Its primary goal is to find the optimal hyperplane that maximizes the distance (the margin) between two or more classes of data points in a multi-dimensional space."
      ],
      "metadata": {
        "id": "r_IRtA2yxPIZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.What is the Kernel Trick in SVM?\n",
        "\n",
        "\n",
        "Ans:In the world of Support Vector Machines (SVM), the Kernel Trick is a clever mathematical shortcut that allows the algorithm to solve non-linear problems without actually doing the heavy lifting of transforming data into higher dimensions."
      ],
      "metadata": {
        "id": "6IKRfyCUxl6a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7.Write a Python program to train two SVM classifiers with Linear and RBF\n",
        "kernels on the Wine dataset, then compare their accuracies.\n",
        "\n",
        "\n",
        "Ans:Training Support Vector Machines (SVM) on the Wine dataset is a classic way to see how different kernels handle multi-class classification. The Linear kernel works best when the data is linearly separable, while the RBF (Radial Basis Function) kernel excels at finding non-linear boundaries."
      ],
      "metadata": {
        "id": "xaZ-NX52x-w7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the Wine dataset\n",
        "wine = datasets.load_wine()\n",
        "X, y = wine.data, wine.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Standardize features (Recommended for SVM)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Train Linear SVM\n",
        "linear_svm = SVC(kernel='linear')\n",
        "linear_svm.fit(X_train, y_train)\n",
        "linear_acc = accuracy_score(y_test, linear_svm.predict(X_test))\n",
        "\n",
        "# Train RBF SVM\n",
        "rbf_svm = SVC(kernel='rbf')\n",
        "rbf_svm.fit(X_train, y_train)\n",
        "rbf_acc = accuracy_score(y_test, rbf_svm.predict(X_test))\n",
        "\n",
        "# Output results\n",
        "print(f\"Linear Kernel Accuracy: {linear_acc:.4f}\")\n",
        "print(f\"RBF Kernel Accuracy:    {rbf_acc:.4f}\")\n",
        "\n",
        "# Comparison logic\n",
        "if linear_acc > rbf_acc:\n",
        "    print(\"Linear kernel performed better.\")\n",
        "elif rbf_acc > linear_acc:\n",
        "    print(\"RBF kernel performed better.\")\n",
        "else:\n",
        "    print(\"Both kernels achieved the same accuracy.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7NJRd8sFxlFy",
        "outputId": "fd201938-4fed-4263-c3b1-d5f20638c5a1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear Kernel Accuracy: 0.9815\n",
            "RBF Kernel Accuracy:    0.9815\n",
            "Both kernels achieved the same accuracy.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8.What is the Naïve Bayes classifier, and why is it called \"Naïve\"?\n",
        "\n",
        "\n",
        "Ans:The Naïve Bayes classifier is a probabilistic machine learning model used for classification tasks. It is built upon Bayes' Theorem, which calculates the probability of a hypothesis (a class) based on prior knowledge (features).\n",
        "\n",
        "\n",
        "The algorithm calculates the probability of each class for a given set of features and selects the class with the highest probability.\n",
        "\n",
        "\n",
        "Why is it called \"Naïve\"?\n",
        "\n",
        "\n",
        "It is called \"Naïve\" because it makes a radical, simplifying assumption: Conditional Independence.\n",
        "\n",
        "\n",
        "Specifically, it assumes that the presence of one particular feature in a class is completely unrelated to the presence of any other feature.\n",
        "\n",
        "\n",
        " - In Reality: Features are often linked. For example, in an email, the word \"Money\" often appears near the word \"Transfer.\"\n",
        " - he \"Naïve\" Assumption: The algorithm treats \"Money\" and \"Transfer\" as if they have zero relationship to each other, calculating their probabilities in total isolation.\n",
        "\n",
        "\n",
        "Despite this \"naïve\" disregard for feature correlation, the classifier is remarkably effective for complex tasks like spam filtering and sentiment analysis."
      ],
      "metadata": {
        "id": "aK24nfpszKnc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9.Explain the differences between Gaussian Naïve Bayes, Multinomial Naïve\n",
        "Bayes, and Bernoulli Naïve Bayes\n",
        "\n",
        "\n",
        "Ans:The primary difference between these three algorithms is the mathematical distribution they assume for the input features.\n",
        "\n",
        "\n",
        "1. Gaussian Naïve Bayes\n",
        "\n",
        "\n",
        "Used when features are continuous (e.g., decimals, measurements). It assumes that the data for each class follows a Normal (Gaussian) distribution.\n",
        "\n",
        "\n",
        " - Assumption: The features follow a bell curve.\n",
        " - Data Example: Predicting weather using temperature ($22.4$°C) and humidity ($65.2$%).\n",
        " - Formula Logic: It calculates the mean ($\\mu$) and variance ($\\sigma^2$) for each class.\n",
        "\n",
        "\n",
        "2. Multinomial Naïve Bayes\n",
        "\n",
        "\n",
        "Used when features represent discrete counts. It is the standard for text classification where you focus on word frequency.\n",
        "\n",
        "\n",
        " - Used when features represent discrete counts. It is the standard for text classification where you focus on word frequency.\n",
        " - Data Example: Word counts in an email (the word \"offer\" appears $5$ times).\n",
        " - Key Behavior: It accounts for how many times a feature appears; $10$ occurrences carry more weight than $1$.\n",
        "\n",
        "\n",
        "\n",
        "3. Bernoulli Naïve Bayes\n",
        "\n",
        "\n",
        "Used when features are binary (0 or 1). It is similar to Multinomial but focuses on whether a feature exists or not.\n",
        "\n",
        "\n",
        " - Assumption: Features are independent booleans.\n",
        " - Data Example: A \"bag of words\" where you only mark if a word is present ($1$) or absent ($0$), regardless of how many times it appears.\n",
        " - Key Behavior: It explicitly penalizes the non-occurrence of a feature, which can be useful in short-text classification."
      ],
      "metadata": {
        "id": "dYhD39o7z3k0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10.Write a Python program to train a Gaussian Naïve Bayes classifier on the Breast Cancer\n",
        "dataset and evaluate accuracy."
      ],
      "metadata": {
        "id": "xEeRLHeP3Dgy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train the Gaussian Naïve Bayes classifier\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions and evaluate accuracy\n",
        "y_pred = gnb.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vu25lH_Wy862",
        "outputId": "22c822c4-1359-4cb4-aa2d-ef8b9ba10287"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9737\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rv8Nu6WS3sDI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}